{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153b5ead",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Full inference script will be refactored and made available closer to publication. The following code is provided to reviewers for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class of all CLIP-like dual encoder models\n",
    "class DualEncoderModel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "    def encode_text(self, token_ids, attention_mask = None):\n",
    "        \"\"\"\n",
    "        encode a sequence text tokens into shared visual/language latent space \n",
    "        optionally accepts an attention mask to prohibit attention to certain positions\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"to be implemented\")\n",
    "    \n",
    "    def encode_image(self, image):\n",
    "        \"\"\"\n",
    "        encode an image into shared visual/language latent space\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"to be implemented\")\n",
    "\n",
    "# create a dummy dual encoder model for illustration purposes\n",
    "class DummyDualEncoderModel(DualEncoderModel):\n",
    "    def __init__(self, emb_dim = 128):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "    def encode_text(self, token_ids, attention_mask = None):\n",
    "        # create random embeddings for illustration purposes\n",
    "        text_emb = torch.rand(len(token_ids), self.emb_dim)\n",
    "        return text_emb\n",
    "    \n",
    "    def encode_image(self, image):\n",
    "        # create random embeddings for illustration purposes\n",
    "        img_emb = torch.rand(len(image), self.emb_dim)\n",
    "        return img_emb\n",
    "    \n",
    "def spatial_smoothing(logits, coords, ss_k = 8):\n",
    "    I = knn_lookup(coords.astype('float32'), k = ss_k, device=logits.device)\n",
    "    logits = logits[I].mean(dim=1)\n",
    "    return logits\n",
    "\n",
    "def knn_lookup(coords, device, k = 8):\n",
    "    d = coords.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(coords)\n",
    "    D, I = index.search(coords, k + 1)\n",
    "    I = I.to(device)\n",
    "    return I\n",
    "\n",
    "def build_zero_shot_classifier(model, tokenizer, classnames, templates, device = 'cuda'):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        for classname in classnames:\n",
    "            texts = [template.replace('CLASSNAME', classname) for template in templates]\n",
    "            texts, attention_mask = tokenize(tokenizer, texts) # Tokenize with custom tokenizer\n",
    "            texts = torch.from_numpy(np.array(texts)).to(device)\n",
    "            attention_mask = torch.from_numpy(np.array(attention_mask)).to(device)\n",
    "            class_embeddings = model.encode_text(texts, attention_mask=attention_mask)\n",
    "            class_embedding = F.normalize(class_embeddings, dim=-1).mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(device)\n",
    "    return zeroshot_weights\n",
    "\n",
    "def tokenize(tokenizer, texts):\n",
    "    tokens = tokenizer.batch_encode_plus(texts, \n",
    "                                         max_length = 64,\n",
    "                                         add_special_tokens = True, \n",
    "                                         return_token_type_ids = False,\n",
    "                                         truncation = True,\n",
    "                                         padding = 'max_length',\n",
    "                                         return_attention_mask=True)\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "def topK_pooling(logits, topK = (1,)):\n",
    "    \"\"\"\n",
    "    logits: N x C logits for each patch\n",
    "    topK: tuple of the top number of patches to use for pooling\n",
    "    \"\"\"\n",
    "    # Sums logits across topj patches for each class, to get class prediction for each topj\n",
    "    maxK = min(max(topK), logits.size(0)) # Ensures k is smaller than number of patches. \n",
    "    values, _ = logits.topk(maxK, 0, True, True) # maxK x C\n",
    "    values = {k : values[:min(k, maxK)].mean(dim=0, keepdim=True) for k in topK} # dict of 1 x C logit scores\n",
    "    preds = {key: val.argmax(dim=1) for key,val in values.items()} # dict of predicted class indices\n",
    "    return preds, values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd69aad",
   "metadata": {},
   "source": [
    "## inference on WSIs\n",
    "Suppose a WSI has 10,000 patches, each 256 x 256, we first embed each patch using the DualEncoderModel's *encode_image* method. For large WSIs, due to GPU memory constraints, these embeddings will be created in mini-batches and cached to local SSD storage before inference can performed. We use an embedding dimension of 512 in our model, as a result, the preprocesed input to MI-Zero for this WSI will be a tensor of size 10,000 x 512.\n",
    "\n",
    "In the example below, for simplicity, we'll assume the WSI is very small and has only 64 patches, therefore we can perform embedding and inference in a single step without caching the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94625a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device \n",
    "device = 'cpu'\n",
    "\n",
    "# embedding dimension\n",
    "emb_dim = 512\n",
    "\n",
    "# classnames\n",
    "classnames = ['lung adenocarcinoma', 'lung squamous cell carcinoma']\n",
    "\n",
    "# list of templates to ensemble\n",
    "templates = ['an image of CLASSNAME.',\n",
    "             'CLASSNAME is present.',\n",
    "             'a histopathological image showing CLASSNAME.',\n",
    "             'presence of CLASSNAME.',\n",
    "             'CLASSNAME.']\n",
    "\n",
    "# whether to perform spatial smoothing\n",
    "ss = True\n",
    "try: \n",
    "    import faiss\n",
    "except ImportError:\n",
    "    print('faiss installation not found, disabling spatial smoothing')\n",
    "    ss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = DummyDualEncoderModel(emb_dim = emb_dim).to(device)\n",
    "\n",
    "# a WSI of 64 256 by 256 images\n",
    "imgs = torch.rand(64, 3, 256, 256).to(device)\n",
    "\n",
    "# coordinates of each patch, used for building knn for spatial smoothing\n",
    "coords = np.random.randint(1000, size = (len(imgs), 2))\n",
    "\n",
    "# a embedded WSI bag of size 64 x emb_dim\n",
    "image_features = model.encode_image(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pretrained tokenizer, for illustration, we use BioClinicalBERT's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', fast=True)\n",
    "# build classifier using prompts\n",
    "classifier = build_zero_shot_classifier(model, tokenizer, classnames, templates, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb40010",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = F.normalize(image_features, dim=-1) \n",
    "logits = image_features @ classifier\n",
    "\n",
    "print('classifier weights: ', tuple(classifier.size()))\n",
    "print('image embeddings: ', tuple(image_features.size()))\n",
    "\n",
    "\n",
    "if ss:\n",
    "    logits = spatial_smoothing(logits, coords)\n",
    "\n",
    "# mean pooling\n",
    "meanpool_logits = logits.mean(dim=0, keepdim=True)\n",
    "\n",
    "# topK pooling\n",
    "k = 10\n",
    "_, topkpool_logits = topK_pooling(logits, topK = (k,))\n",
    "topkpool_logits = topkpool_logits[k]\n",
    "\n",
    "print('\\ninstance logits: ', tuple(logits.size()))\n",
    "print('mean pooled logits: ', tuple(meanpool_logits.size()))\n",
    "print('topk pooled logits: ', tuple(topkpool_logits.size()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
